# ğŸ‡»ğŸ‡³ Vietnamese ASR â€“ Dialect-Aware Speech Recognition

Fine-tuned Whisper-based model on **ViMD Dataset (63 provinces, 3 dialects)**

---

## ğŸ“Œ Overview

This repository contains the complete pipeline for building a **Vietnamese Automatic Speech Recognition (ASR)** system specialized for **regional dialects**.
The project includes:

* ğŸ”§ **Full preprocessing + training pipeline** for fine-tuning Whisper/PhoWhisper
* ğŸ§ª **Evaluation framework** (coming in the next folder)
* ğŸ–¥ï¸ **Streamlit demo UI** for quick inference
* ğŸ“¦ **Model conversion utilities** for deployment (CT2 / ONNX / HuggingFace format)
* ğŸš€ Ready-to-run scripts for VastAI, Google Drive, and local machines

This project is built for the **FPT University DSP391m Capstone**, with a strong focus on real-world ASR performance across dialects.

---

## ğŸ“ Repository Structure

```
Vietnamese_ASR/
â”‚
â”œâ”€â”€ demo/                     # Streamlit demo interface
â”‚   â””â”€â”€ demo.py
â”‚
â”œâ”€â”€ fine_tune_model/          # This folder includes model weights and tokenizers, hÃ¡ to be downloaded from Google Drive
â”‚   â””â”€â”€ (copy model files from Google Drive here)
â”‚
â”œâ”€â”€ evaluation/               # (Sáº½ thÃªm) Evaluation scripts for comparing models
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ convert_model.py          # Convert model â†’ CT2, ONNX, HF format
â”œâ”€â”€ train.py                  # Training / fine-tuning script
â”‚
â”œâ”€â”€ setup.sh                  # Environment setup for VastAI / Linux
â”œâ”€â”€ setup_data.sh             # Download + extract processed ViMD dataset
â”‚
â”œâ”€â”€ requirement.txt
â””â”€â”€ README.md                 # (this file)
```

---

## ğŸ”§ Installation

### 1ï¸âƒ£ Clone repo

```bash
git clone https://github.com/<your_repo>/Vietnamese_ASR.git
cd Vietnamese_ASR
```

### 2ï¸âƒ£ Create environment

Use conda or venv:

```bash
bash setup.sh
```

Or manually:

```bash
pip install -r requirement.txt
```

---

## ğŸ“¥ Prepare Model Files

Your teammate provides a Google Drive folder containing:

```
train_outputs/
â””â”€â”€ phowhisper_vimd/
    â””â”€â”€ ctranslate2_model/
```

Copy toÃ n bá»™ files trong `ctranslate2_model/` vÃ o:

```
Vietnamese_ASR/fine_tune_model/
```

---

## ğŸ§ Streamlit Demo

### 1ï¸âƒ£ Go to demo folder

```bash
cd demo
```

### 2ï¸âƒ£ Run demo

```bash
streamlit run demo.py
```

Sau Ä‘Ã³ truy cáº­p:
ğŸ‘‰ [http://localhost:8501](http://localhost:8501)

---

## ğŸ‹ï¸ Training

### 1ï¸âƒ£ Prepare dataset

Processed ViMD dataset stored on Google Drive.

Run:

```bash
bash setup_data.sh
```

This script will:

* Mount or download from Google Drive
* Extract dataset
* Organize into `train/` â€“ `valid/` â€“ `test/` folders

### 2ï¸âƒ£ Start fine-tuning

```bash
python train.py --config configs/vimd_config.yaml
```

Training script includes:

* Augmentation
* Mixed precision
* Gradient accumulation
* Checkpoint saving
* Logging (loss, WER, CER)

---

## ğŸ”„ Model Conversion

To convert the fine-tuned model into **CTranslate2** for fast inference:

```bash
python convert_model.py --source <path_to_model> --output fine_tune_model/
```

Supports:

* CTranslate2
* HuggingFace
* ONNX (coming soon)

---

## ğŸ§ª Evaluation (Upcoming Folder)

A new folder `/evaluation` will contain:

* ğŸ“Š Compare Whisper base vs large vs PhoWhisper vs your fine-tuned model
* ğŸ·ï¸ Evaluate per dialect: North / Central / South
* ğŸ… Compute WER / CER / Speaker-level performance
* ğŸ”‰ Noise robustness evaluation
* ğŸ“ˆ Visualizations (confusion matrix, error samples)

Example (coming soon):

```
evaluation/
â”‚   evaluate_ct2.py
â”‚   evaluate_hf.py
â”‚   compare_models.ipynb
â”‚   dialect_breakdown.csv
```

---

## ğŸ§  Model Details

* Base model: **PhoWhisper** (Vietnamese-specialized Whisper variant)
* Fine-tuning dataset: **ViMD â€“ 102.5 hours â€“ 63 provinces**
* Tokenizer: SentencePiece
* Feature extractor: 80-channel Mel-spectrogram
* Optimizer: AdamW
* Metrics: WER / CER (character-level suited for Vietnamese)

---

## ğŸ—‚ Dataset

We use **ViMD**, a large-scale Vietnamese dialect dataset:

| Region  | Provinces | %   |
| ------- | --------- | --- |
| North   | 25        | 40% |
| Central | 19        | 30% |
| South   | 19        | 30% |

Includes:

* 1.5M text characters
* 80k+ spoken utterances
* Natural speech (non-studio)
* Full demographic metadata

---

## ğŸš€ Deployment (Future Work)

Planned additions:

* FastAPI real-time ASR server
* gRPC service
* Mobile-ready model export
* Websocket streaming

---

## ğŸ¤ Contributors

* **Thuáº­n HoÃ ng** â€“ AI Engineer
* **Khoa ChÃ¢u** â€“ Model Training / Demo
* **ViMD Team** â€“ Dataset providers
* FPT University â€“ Faculty of AI & DS

---

## ğŸ“„ License

MIT License
(Feel free to use, modify, and cite our work.)

---

## ğŸ“¬ Contact

For questions or collaboration:

ğŸ“§ **[kodtt1234@gmail.com](mailto:kodtt1234@gmail.com)**

---

Náº¿u báº¡n muá»‘n, tÃ´i cÃ³ thá»ƒ thÃªm:

âœ… Badges (Python version, license, model size, WER score)
âœ… ThÃªm hÃ¬nh minh há»a kiáº¿n trÃºc Whisper
âœ… Banner Ä‘áº¹p cho GitHub
âœ… Táº¡o â€œdemo videoâ€ hÆ°á»›ng dáº«n trong README

Báº¡n muá»‘n má»Ÿ rá»™ng README theo hÆ°á»›ng nÃ o?
